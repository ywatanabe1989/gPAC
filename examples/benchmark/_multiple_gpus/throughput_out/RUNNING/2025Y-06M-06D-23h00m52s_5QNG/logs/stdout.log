[94m
########################################
## mngs v1.11.0
## 2025Y-06M-06D-23h00m52s_5QNG (PID: 261636)
########################################
[0m
[93m
========================================
./examples/performance/multiple_gpus/throughput.py
Namespace(n_perm=0)
========================================
[0m
üîß GPU Count: 4
üîß GPU Memory: 79.1 GiB per GPU
üîß Total VRAM: 316.6 GiB
   GPU 0: NVIDIA A100 80GB PCIe
   GPU 1: NVIDIA A100 80GB PCIe
   GPU 2: NVIDIA A100 80GB PCIe
   GPU 3: NVIDIA A100 80GB PCIe

üìã Test Configuration:
   batch_size: 8
   n_channels: 16
   seq_sec: 8.0s (sequence duration)
   n_perm: 0
   seq_len: 4096 samples (computed from seq_sec * fs)
   fs: 512

============================================================
üöÄ Throughput Scaling Test
üìã Multi-GPU throughput across different configurations
============================================================
üîç Finding optimal single GPU batch size...
   Testing batch sizes for optimal throughput...
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.002 GiB
   Batch 8: 19.7 samples/sec (0.41s)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.004 GiB
   Batch 16: 24.9 samples/sec (0.64s)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.006 GiB
   Batch 24: 24.7 samples/sec (0.97s)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.008 GiB
   Batch 32: 25.3 samples/sec (1.27s)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.012 GiB
   Batch 48: 25.4 samples/sec (1.89s)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.016 GiB
   Batch 64: 25.5 samples/sec (2.51s)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.024 GiB
   Batch 96: 25.6 samples/sec (3.75s)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.031 GiB
   Batch 128: 25.6 samples/sec (5.00s)
üéØ Optimal single GPU batch: 128 (25.6 samples/sec)

============================================================
üîß Testing with 1 GPU(s)
============================================================

--- 1 GPU(s): Small (64) ---
Data shape: torch.Size([64, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.016 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.017 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.019 GiB
‚ùå 1 GPU(s): ERROR - name 'np' is not defined

--- 1 GPU(s): Optimal (128) ---
Data shape: torch.Size([128, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.031 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.034 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.037 GiB
‚ùå 1 GPU(s): ERROR - name 'np' is not defined

--- 1 GPU(s): 2x (256) ---
Data shape: torch.Size([256, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.063 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.069 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.076 GiB
‚ùå 1 GPU(s): ERROR - name 'np' is not defined

--- 1 GPU(s): 4x (512) ---
Data shape: torch.Size([512, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
‚ùå 1 GPU(s): FAILED

============================================================
üîß Testing with 2 GPU(s)
============================================================

--- 2 GPU(s): Small (64) ---
Data shape: torch.Size([64, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.016 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.017 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.017 GiB
‚ùå 2 GPU(s): ERROR - name 'np' is not defined

--- 2 GPU(s): Optimal (128) ---
Data shape: torch.Size([128, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.031 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.034 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.037 GiB
‚ùå 2 GPU(s): ERROR - name 'np' is not defined

--- 2 GPU(s): 2x (256) ---
Data shape: torch.Size([256, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.063 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.069 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.076 GiB
‚ùå 2 GPU(s): ERROR - name 'np' is not defined

--- 2 GPU(s): 4x (512) ---
Data shape: torch.Size([512, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
‚ùå 2 GPU(s): FAILED

============================================================
üîß Testing with 3 GPU(s)
============================================================

--- 3 GPU(s): Small (64) ---
Data shape: torch.Size([64, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.016 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.017 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.019 GiB
‚ùå 3 GPU(s): ERROR - name 'np' is not defined

--- 3 GPU(s): Optimal (128) ---
Data shape: torch.Size([128, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.031 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.034 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.037 GiB
‚ùå 3 GPU(s): ERROR - name 'np' is not defined

--- 3 GPU(s): 2x (256) ---
Data shape: torch.Size([256, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.063 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.069 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.076 GiB
‚ùå 3 GPU(s): ERROR - name 'np' is not defined

--- 3 GPU(s): 4x (512) ---
Data shape: torch.Size([512, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
‚ùå 3 GPU(s): FAILED

============================================================
üîß Testing with 4 GPU(s)
============================================================

--- 4 GPU(s): Small (64) ---
Data shape: torch.Size([64, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.016 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.017 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.019 GiB
‚ùå 4 GPU(s): ERROR - name 'np' is not defined

--- 4 GPU(s): Optimal (128) ---
Data shape: torch.Size([128, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.031 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.034 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.037 GiB
‚ùå 4 GPU(s): ERROR - name 'np' is not defined

--- 4 GPU(s): 2x (256) ---
Data shape: torch.Size([256, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.063 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.069 GiB
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.076 GiB
‚ùå 4 GPU(s): ERROR - name 'np' is not defined

--- 4 GPU(s): 4x (512) ---
Data shape: torch.Size([512, 16, 4096])
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
Execution failed: CUDA out of memory. Tried to allocate 25.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 13.51 GiB is free. Including non-PyTorch memory, this process has 65.62 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
   DEBUG: PAC model device: cuda:0
   DEBUG: Memory after data transfer: 0.125 GiB
