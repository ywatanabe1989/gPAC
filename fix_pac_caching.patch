--- a/src/gpac/_PAC.py
+++ b/src/gpac/_PAC.py
@@ -20,6 +20,8 @@ Ultra-high-speed PAC for 80GB VRAM x4 GPU nodes:
 
 import torch
 import torch.nn as nn
+import hashlib
+import numpy as np
 
 from .core._BandPassFilter import BandPassFilter
 from .core._Hilbert import Hilbert
@@ -53,6 +55,7 @@ class PAC(nn.Module):
         amp_n_pool_ratio: Optional[float] = None,
         temperature: float = 1.0,
         hard_selection: bool = False,
+        cache_size_limit: int = 100,  # Limit cache size
     ):
         # Parameter Validation
         if seq_len <= 0:
@@ -72,6 +75,7 @@ class PAC(nn.Module):
         self.enable_caching = enable_caching
         self.surrogate_chunk_size = surrogate_chunk_size
         self.trainable = trainable
+        self.cache_size_limit = cache_size_limit
 
         # Devices
         if device_ids == "all" and torch.cuda.is_available():
@@ -137,6 +141,42 @@ class PAC(nn.Module):
         """Amplitude frequency bands as tensor (n_bands, 2) with [low, high] Hz."""
         return self.bandpass.amp_bands_hz
 
+    def _compute_tensor_hash(self, x: torch.Tensor) -> str:
+        """Compute a hash of tensor content for caching.
+        
+        Uses a subset of the tensor data to compute hash efficiently.
+        """
+        # Convert to CPU numpy for consistent hashing
+        x_np = x.detach().cpu().numpy()
+        
+        # Use a subset of data for efficiency (first, middle, last parts)
+        shape = x_np.shape
+        flat = x_np.flatten()
+        n_elements = flat.size
+        
+        # Sample elements: first 100, middle 100, last 100, plus shape info
+        sample_size = min(100, n_elements // 3)
+        if n_elements > 300:
+            indices = np.concatenate([
+                np.arange(sample_size),  # First elements
+                np.arange(n_elements // 2 - sample_size // 2, 
+                         n_elements // 2 + sample_size // 2),  # Middle elements
+                np.arange(n_elements - sample_size, n_elements)  # Last elements
+            ])
+            sample = flat[indices]
+        else:
+            sample = flat
+        
+        # Include shape, dtype, and statistics in hash
+        hash_data = np.concatenate([
+            sample,
+            np.array(shape),
+            np.array([np.mean(x_np), np.std(x_np), np.min(x_np), np.max(x_np)])
+        ])
+        
+        # Compute hash
+        return hashlib.md5(hash_data.tobytes()).hexdigest()
+
     def forward(self, x: torch.Tensor, compute_distributions: bool = False) -> Dict[str, torch.Tensor]:
         if x.dim() == 3:
             x = x.unsqueeze(2)
@@ -147,11 +187,19 @@ class PAC(nn.Module):
             raise ValueError(f"Input must be 3D or 4D, got {x.dim()}D")
 
         if self.enable_caching and self.result_cache is not None:
-            cache_key = (x.shape, x.device, x.dtype, x.data_ptr())
+            # Use content-based hash instead of pointer
+            cache_key = self._compute_tensor_hash(x)
             if cache_key in self.result_cache:
                 result = self.result_cache[cache_key]
                 if squeeze_segments:
                     result = self._squeeze_segment_dim(result)
                 return result
+            
+            # Limit cache size
+            if len(self.result_cache) >= self.cache_size_limit:
+                # Remove oldest entry (FIFO)
+                oldest_key = next(iter(self.result_cache))
+                del self.result_cache[oldest_key]
 
         if x.device != self.device: