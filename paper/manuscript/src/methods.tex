%% -*- mode: latex -*-
%% Methods section for gPAC manuscript

\section{Methods}

\subsection{Synthetic Data Generation}
We generated synthetic neural signals with known phase-amplitude coupling characteristics to validate computational accuracy and benchmark performance. The synthetic data generation followed established methods \citep{Tort2010} with modifications for GPU optimization. Each synthetic signal comprised a low-frequency carrier wave modulated by high-frequency bursts:

\begin{equation}
x(t) = \sin(2\pi f_{\text{phase}} t) + A(t) \cdot \sin(2\pi f_{\text{amp}} t) + \epsilon(t)
\end{equation}

where $f_{\text{phase}}$ represents the phase frequency (4-30 Hz), $f_{\text{amp}}$ denotes the amplitude frequency (30-200 Hz), $A(t)$ is the amplitude modulation envelope coupled to the phase of the low-frequency component, and $\epsilon(t)$ represents Gaussian noise. The coupling strength was systematically varied from 0 (no coupling) to 1 (perfect coupling) to evaluate sensitivity across different signal-to-noise ratios.

\subsection{Validation Datasets}
To ensure real-world applicability, we validated gPAC using publicly available electrophysiological recordings. While the current validation focuses on synthetic data with ground truth coupling, the framework is designed to handle multi-channel recordings from various modalities including EEG, MEG, and intracranial recordings.

\subsection{Implementation of GPU-accelerated PAC}
We developed gPAC (GPU-accelerated Phase-Amplitude Coupling), a PyTorch-based framework that leverages parallel computing capabilities of modern GPUs. The implementation comprises three core modules: (1) BandPassFilter for frequency decomposition, (2) Hilbert transform for analytic signal computation, and (3) ModulationIndex for PAC quantification. The package is publicly available on PyPI (\texttt{pip install gpu-pac}) and GitHub (https://github.com/ywatanabe1989/gPAC).

\subsubsection{Bandpass Filtering}
The BandPassFilter module implements finite impulse response (FIR) filters using differentiable operations. Unlike traditional implementations, gPAC offers both static and trainable filter configurations. The static mode uses fixed frequency bands, while the trainable mode enables data-driven optimization of frequency boundaries through gradient descent. Filter coefficients are computed using the window method with a Hamming window:

\begin{equation}
h[n] = w[n] \cdot \text{sinc}\left(\frac{2f_c}{f_s}(n - \frac{N-1}{2})\right)
\end{equation}

where $w[n]$ is the window function, $f_c$ is the cutoff frequency, $f_s$ is the sampling rate, and $N$ is the filter order.

\subsubsection{Hilbert Transform}
The Hilbert module computes the analytic signal using a differentiable approximation suitable for backpropagation. Instead of the traditional FFT-based approach, we implement a sigmoid-based approximation that maintains gradient flow:

\begin{equation}
\mathcal{H}\{x(t)\} \approx x(t) \ast h_{\text{sigmoid}}(t)
\end{equation}

where $h_{\text{sigmoid}}(t)$ is a learnable kernel that approximates the Hilbert transform response.

\subsubsection{Usage Example}
gPAC provides a simple API for PAC computation:
\begin{verbatim}
import gpac
import torch

# Initialize PAC calculator
pac = gpac.PAC(
    seq_len=1024,
    fs=500,
    pha_range_hz=(4, 30),
    amp_range_hz=(30, 200),
    pha_n_bands=10,
    amp_n_bands=10,
    n_perm=100
)

# Compute PAC
signal = torch.randn(1, 8, 1024)  # (batch, channels, time)
result = pac(signal)
pac_values = result['pac']  # Shape: (1, 8, 10, 10)
z_scores = result['z_score']  # With surrogate testing
\end{verbatim}

\subsection{Computational Environment}
Benchmarking experiments were conducted on a high-performance workstation with the following specifications:

\begin{itemize}
\item \textbf{CPU}: AMD Ryzen 9 7950X (16 cores, 32 threads, 4.5 GHz base / 5.7 GHz boost)
\item \textbf{GPU}: NVIDIA GeForce RTX 4090 (24 GB VRAM, 16384 CUDA cores)
\item \textbf{Memory}: 64 GB DDR5-5600
\item \textbf{Software}: PyTorch 2.0+, CUDA 12.1, Python 3.10
\end{itemize}

For multi-GPU experiments, we utilized a cluster with 8Ã— NVIDIA A100 GPUs (40 GB each) to demonstrate scalability. All experiments were repeated five times to ensure reproducibility, with median values reported.

\subsection{Validation Against Established Methods}
We validated gPAC against TensorPAC \citep{Combrisson2020}, a widely-used CPU-based implementation. The validation comprised three components:

\subsubsection{Numerical Accuracy}
We computed the Pearson correlation coefficient and mean absolute error (MAE) between gPAC and TensorPAC outputs across identical input signals. For fair comparison, we ensured both implementations used:
\begin{itemize}
\item Identical frequency band definitions
\item Modulation Index (MI) as the coupling metric (Tort et al., 2010)
\item Amplitude time-shifting for surrogate generation
\item 200 permutations for z-score normalization
\end{itemize}

\subsubsection{Statistical Validation}
We generated 1000 synthetic signals with varying coupling strengths (MI = 0 to 0.5) and compared the detected coupling patterns. Both methods were evaluated on their ability to:
\begin{itemize}
\item Detect true coupling (sensitivity)
\item Reject spurious coupling (specificity)
\item Accurately estimate coupling strength
\end{itemize}

\subsection{Performance Benchmarking}
Performance benchmarking was conducted across a comprehensive parameter space to characterize scaling behavior:

\begin{itemize}
\item \textbf{Data dimensions}: Signal length (256-16384 samples), number of channels (1-256), batch size (1-128)
\item \textbf{Frequency parameters}: Phase bands (2-50 Hz, 1-50 bands), amplitude bands (30-200 Hz, 1-50 bands)
\item \textbf{Computational parameters}: Number of permutations (0-1000), precision (FP16/FP32), device (CPU/GPU/Multi-GPU)
\item \textbf{Optimization settings}: Gradient computation (on/off), trainable filters (on/off), memory optimization (on/off)
\end{itemize}

Each configuration was tested with five independent runs, measuring:
\begin{itemize}
\item Total computation time (including data transfer)
\item GPU memory usage
\item Numerical accuracy compared to reference implementation
\item Speedup factor relative to CPU baseline
\end{itemize}

\subsection{Trainable PAC Analysis}
To demonstrate the differentiable nature of gPAC, we implemented an end-to-end trainable system for optimizing frequency band selection. The optimization objective was to maximize coupling strength while maintaining physiological plausibility:

\begin{equation}
\mathcal{L} = -\text{MI}(\phi_{\text{low}}, A_{\text{high}}) + \lambda \cdot \mathcal{R}(\theta)
\end{equation}

where MI is the modulation index, $\phi_{\text{low}}$ and $A_{\text{high}}$ are the phase and amplitude components, $\lambda$ is a regularization weight, and $\mathcal{R}(\theta)$ constrains the frequency band parameters $\theta$ to physiologically relevant ranges.

\subsection{Statistical Analysis}
Statistical comparisons between gPAC and reference implementations employed non-parametric tests due to non-normal distributions of computation times. The Wilcoxon signed-rank test assessed paired differences in execution time, while the Mann-Whitney U test evaluated accuracy metrics. Correlation analyses used Spearman's rank correlation to account for non-linear relationships. All statistical tests used $\alpha = 0.05$ with Bonferroni correction for multiple comparisons.

\label{sec:methods}